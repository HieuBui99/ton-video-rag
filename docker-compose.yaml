services:
  llamacpp-server:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    ports:
      - 8080:8080
    volumes:
      - ./models:/models
    environment:
      # alternatively, you can use "LLAMA_ARG_MODEL_URL" to download the model
      LLAMA_ARG_MODEL: /models/Qwen3VL-8B-Instruct-Q4_K_M.gguf
      LLAMA_ARG_MMPROJ: /models/mmproj-Qwen3VL-8B-Instruct-F16.gguf
      LLAMA_ARG_N_PARALLEL: 2
      LLAMA_ARG_PORT: 8080
      LLAMA_API_KEY: "admin"

    command: >
      --n-gpu-layers 64
      --flash-attn on
      --jinja
      --top-p 0.95
      --top-k 20
      --temp 0.7
      --presence-penalty 1.5
      --ctx-size 12000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
  vectorizer:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-ipex-latest
    ports:
      - "8081:80"
    volumes:
      - "./models:/data"
    command: ["--model-id", "Qwen/Qwen3-Embedding-0.6B", "--auto-truncate"]